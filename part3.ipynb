{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "993ce327",
   "metadata": {},
   "source": [
    "# RETAIN - Reverse Time Attention Model\n",
    "\n",
    "## 1. Overview\n",
    "\n",
    "\n",
    "This notebook implements **RETAIN**, a recurrent neural network with reverse-time attention for interpretable healthcare predictions, allowing us to identify which medical events most influence the model's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cd37976",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# Set seed\n",
    "seed = 24\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "# Define data path\n",
    "DATA_PATH = \"data2\"\n",
    "\n",
    "# Suppress warnings about ragged sequences\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', message='.*creating an ndarray from ragged nested sequences.*')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1272ba55",
   "metadata": {},
   "source": [
    "### About Raw Data\n",
    "\n",
    "- Dataset: Synthetic data based on [MIMIC-III](https://mimic.physionet.org/gettingstarted/access/)  \n",
    "- Input: Sequences of diagnosis codes for each patient  \n",
    "- Task: Predict Heart Failure occurrence  \n",
    "- Data is already preprocessed and ready to be loaded into the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d534676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of patients (pids): 1000\n",
      "Number of visits (vids): 1000\n",
      "Number of heart failure labels (hfs): 1000\n",
      "Number of sequences (seqs): 1000\n",
      "Number of diagnosis types (types): 619\n",
      "Number of related types (rtypes): 619\n"
     ]
    }
   ],
   "source": [
    "pids = pickle.load(open(os.path.join(DATA_PATH,'train/pids.pkl'), 'rb')) \n",
    "vids = pickle.load(open(os.path.join(DATA_PATH,'train/vids.pkl'), 'rb'))\n",
    "hfs = pickle.load(open(os.path.join(DATA_PATH,'train/hfs.pkl'), 'rb'))\n",
    "seqs = pickle.load(open(os.path.join(DATA_PATH,'train/seqs.pkl'), 'rb'))\n",
    "types = pickle.load(open(os.path.join(DATA_PATH,'train/types.pkl'), 'rb'))\n",
    "rtypes = pickle.load(open(os.path.join(DATA_PATH,'train/rtypes.pkl'), 'rb'))\n",
    "\n",
    "# Print lengths to verify\n",
    "print(\"Number of patients (pids):\", len(pids))\n",
    "print(\"Number of visits (vids):\", len(vids))\n",
    "print(\"Number of heart failure labels (hfs):\", len(hfs))\n",
    "print(\"Number of sequences (seqs):\", len(seqs))\n",
    "print(\"Number of diagnosis types (types):\", len(types))\n",
    "print(\"Number of related types (rtypes):\", len(rtypes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763e6b08",
   "metadata": {},
   "source": [
    "where\n",
    "\n",
    "- `pids`: contains the patient ids\n",
    "- `vids`: contains a list of visit ids for each patient\n",
    "- `hfs`: contains the heart failure label (0: normal, 1: heart failure) for each patient\n",
    "- `seqs`: contains a list of visit (in ICD9 codes) for each patient\n",
    "- `types`: contains the map from ICD9 codes to ICD-9 labels\n",
    "- `rtypes`: contains the map from ICD9 labels to ICD9 codes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6b9c5e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patient ID: 47537\n",
      "Heart Failure: 0\n",
      "# of visits: 2\n",
      "\t0-th visit id: 0\n",
      "\t0-th visit diagnosis labels: [12, 103, 262, 285, 290, 292, 359, 416, 39, 225, 275, 294, 326, 267, 93]\n",
      "\t0-th visit diagnosis codes: ['DIAG_041', 'DIAG_276', 'DIAG_518', 'DIAG_560', 'DIAG_567', 'DIAG_569', 'DIAG_707', 'DIAG_785', 'DIAG_155', 'DIAG_456', 'DIAG_537', 'DIAG_571', 'DIAG_608', 'DIAG_529', 'DIAG_263']\n",
      "\t1-th visit id: 1\n",
      "\t1-th visit diagnosis labels: [12, 103, 240, 262, 290, 292, 319, 359, 510, 513, 577, 307, 8, 280, 18, 131]\n",
      "\t1-th visit diagnosis codes: ['DIAG_041', 'DIAG_276', 'DIAG_482', 'DIAG_518', 'DIAG_567', 'DIAG_569', 'DIAG_599', 'DIAG_707', 'DIAG_995', 'DIAG_998', 'DIAG_V09', 'DIAG_584', 'DIAG_031', 'DIAG_553', 'DIAG_070', 'DIAG_305']\n"
     ]
    }
   ],
   "source": [
    "# take the 3rd patient as an example\n",
    "\n",
    "print(\"Patient ID:\", pids[3])\n",
    "print(\"Heart Failure:\", hfs[3])\n",
    "print(\"# of visits:\", len(vids[3]))\n",
    "for visit in range(len(vids[3])):\n",
    "    print(f\"\\t{visit}-th visit id:\", vids[3][visit])\n",
    "    print(f\"\\t{visit}-th visit diagnosis labels:\", seqs[3][visit])\n",
    "    print(f\"\\t{visit}-th visit diagnosis codes:\", [rtypes[label] for label in seqs[3][visit]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa2169a",
   "metadata": {},
   "source": [
    "### Data Overview\n",
    "\n",
    "- `seqs` is a **3-level nested list**:  \n",
    "  - `seqs[i][j][k]` gives the **k-th diagnosis code** for the **j-th visit** of the **i-th patient**.  \n",
    "  - Example: `seqs[0][0]` → diagnosis codes of the **first visit of the first patient**.  \n",
    "  - ICD9 codes like `DIAG_276` can be looked up online (e.g., *disorders of fluid electrolyte and acid-base balance*).\n",
    "\n",
    "- `hfs` is a **list of heart failure labels**:  \n",
    "  - `1` → patient has heart failure  \n",
    "  - `0` → patient does not have heart failure  \n",
    "\n",
    "- **Number of heart failure patients in the training set:**  \n",
    "  - `sum(hfs)` gives the total number of HF patients  \n",
    "  - Fraction of HF patients: `sum(hfs) / len(hfs)`  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "658759da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of heart failure patients: 548\n",
      "ratio of heart failure patients: 0.55\n"
     ]
    }
   ],
   "source": [
    "print(\"number of heart failure patients:\", sum(hfs))\n",
    "print(\"ratio of heart failure patients: %.2f\" % (sum(hfs) / len(hfs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05aec5e8",
   "metadata": {},
   "source": [
    "## 2. Build the dataset\n",
    "\n",
    "### - CustomDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "157a3861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of patients in dataset: 1000\n",
      "Example patient first visit: [[85, 112, 346, 380, 269, 511, 114, 103, 530, 597, 511], [85, 103, 112, 513, 511, 19, 149, 530, 186, 66]]\n",
      "Example patient label: 1\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, seqs, hfs):\n",
    "        \"\"\"\n",
    "        Store seqs and hfs as lists. Do NOT convert to np.array since sequences are ragged.\n",
    "        \"\"\"\n",
    "        self.x = seqs  # keep as list\n",
    "        self.y = hfs\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        x = self.x[index]\n",
    "        y = self.y[index]\n",
    "        return x, y\n",
    "\n",
    "# Create dataset instance\n",
    "dataset = CustomDataset(seqs, hfs)\n",
    "\n",
    "# Quick check\n",
    "print(\"Number of patients in dataset:\", len(dataset))\n",
    "print(\"Example patient first visit:\", dataset[0][0])\n",
    "print(\"Example patient label:\", dataset[0][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89e55c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "dataset = CustomDataset(seqs, hfs)\n",
    "\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5b9b36",
   "metadata": {},
   "source": [
    "### Collate Function\n",
    "\n",
    "In this section, we define a **collate function (`collate_fn`)** for batch training RNNs on patient diagnosis sequences.\n",
    "\n",
    "**Key points:**\n",
    "\n",
    "1. **Variable-length sequences:**  \n",
    "   - Each patient has multiple visits, each with a varying number of diagnosis codes.  \n",
    "   - Example: `seqs[i][j][k]` → k-th code of j-th visit of i-th patient.\n",
    "\n",
    "2. **Padding:**  \n",
    "   - Pad visits and diagnosis codes with `0` so all patients in a batch have the same shape.  \n",
    "   - Ensures tensor compatibility for batch processing.\n",
    "\n",
    "3. **Masking:**  \n",
    "   - Create a mask with `1` for original codes and `0` for padded values.  \n",
    "   - Allows the model to **ignore padded values** during training.\n",
    "\n",
    "4. **Reversed sequences:**  \n",
    "   - Flip visits in time (only true visits) for bi-directional RNNs.  \n",
    "   - Create a reversed mask correspondingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4eddd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    \"\"\"\n",
    "    Collate a list of samples into a batch for RNN training.\n",
    "\n",
    "    Args:\n",
    "        data: list of samples from CustomDataset, each (sequence, label)\n",
    "    \n",
    "    Returns:\n",
    "        x: tensor (#patients, max_visits, max_codes), type=torch.long\n",
    "        masks: tensor (#patients, max_visits, max_codes), type=torch.bool\n",
    "        rev_x: same as x but reversed in time\n",
    "        rev_masks: same as masks but reversed in time\n",
    "        y: tensor (#patients), type=torch.float\n",
    "    \"\"\"\n",
    "    sequences, labels = zip(*data)\n",
    "    y = torch.tensor(labels, dtype=torch.float)\n",
    "    \n",
    "    num_patients = len(sequences)\n",
    "    max_num_visits = max(len(patient) for patient in sequences)\n",
    "    max_num_codes = max(len(visit) for patient in sequences for visit in patient)\n",
    "    \n",
    "    x = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.long)\n",
    "    rev_x = torch.zeros_like(x)\n",
    "    masks = torch.zeros_like(x, dtype=torch.bool)\n",
    "    rev_masks = torch.zeros_like(x, dtype=torch.bool)\n",
    "    \n",
    "    for i_patient, patient in enumerate(sequences):\n",
    "        for j_visit, visit in enumerate(patient):\n",
    "            visit_tensor = torch.tensor(visit, dtype=torch.long)\n",
    "            x[i_patient, j_visit, :len(visit)] = visit_tensor\n",
    "            rev_x[i_patient, len(patient)-j_visit-1, :len(visit)] = visit_tensor\n",
    "            masks[i_patient, j_visit, :len(visit)] = True\n",
    "            rev_masks[i_patient, len(patient)-j_visit-1, :len(visit)] = True\n",
    "    \n",
    "    return x, masks, rev_x, rev_masks, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b85f2f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n",
      "torch.float32\n",
      "torch.bool\n",
      "torch.Size([10, 3, 24])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=10, collate_fn=collate_fn)\n",
    "loader_iter = iter(loader)\n",
    "x, masks, rev_x, rev_masks, y = next(loader_iter)\n",
    "\n",
    "print(x.dtype)\n",
    "print(y.dtype)\n",
    "print(masks.dtype)\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe4f1ad",
   "metadata": {},
   "source": [
    "Now we have `CustomDataset` and `collate_fn()`. I split the dataset into training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34c9db88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train dataset: 800\n",
      "Length of val dataset: 200\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "split = int(len(dataset)*0.8)\n",
    "\n",
    "lengths = [split, len(dataset) - split]\n",
    "train_dataset, val_dataset = random_split(dataset, lengths)\n",
    "\n",
    "print(\"Length of train dataset:\", len(train_dataset))\n",
    "print(\"Length of val dataset:\", len(val_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef60ff76",
   "metadata": {},
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57517b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def load_data(train_dataset, val_dataset, collate_fn):\n",
    "    \n",
    "    batch_size = 32\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=collate_fn)\n",
    "    \n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "train_loader, val_loader = load_data(train_dataset, val_dataset, collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab27c84",
   "metadata": {},
   "source": [
    "## 3. RETAIN\n",
    "\n",
    "RETAIN is essentially a RNN model with attention mechanism.\n",
    " \n",
    "The idea of attention is quite simple: it boils down to weighted averaging. Let us consider machine translation in class as an example. When generating a translation of a source text, we first pass the source text through an encoder (an LSTM or an equivalent model) to obtain a sequence of encoder hidden states $\\boldsymbol{h}_1, \\dots, \\boldsymbol{h}_T$. Then, at each step of generating a translation (decoding), we selectively attend to these encoder hidden states, that is, we construct a context vector $\\boldsymbol{c}_i$ that is a weighted average of encoder hidden states.\n",
    "\n",
    "$$\\boldsymbol{c}_i = \\underset{j}{\\Sigma} a_{ij}\\boldsymbol{h}_j$$\n",
    "\n",
    "We choose the weights $a_{ij}$ based both on encoder hidden states $\\boldsymbol{h}_1, \\dots, \\boldsymbol{h}_T$ and decoder hidden states $\\boldsymbol{s}_1, \\dots, \\boldsymbol{s}_T$ and normalize them so that they encode a categorical probability distribution $p(\\boldsymbol{h}_j | \\boldsymbol{s}_i)$.\n",
    "\n",
    "$$\\boldsymbol{a}_{i} = \\text{Softmax}\\left( a(\\boldsymbol{s}_i, \\boldsymbol{h}_j) \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd1bab6",
   "metadata": {},
   "source": [
    "RETAIN has two different attention mechanisms. \n",
    "- One is to help figure out what are the important visits. This attention $\\alpha_i$, which is scalar for the i-th visit, tells you the importance of the i-th visit.\n",
    "- Then we have another similar attention mechanism. But in this case, this attention ways $\\mathbf{\\beta}_i$ is a vector. That gives us a more detailed view of underlying cause of the input. That is, which are the important features within a visit.\n",
    "\n",
    "<img src=./img3/retain-1.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced778cb",
   "metadata": {},
   "source": [
    "Unfolded view of RETAIN’s architecture: Given input sequence $\\mathbf{x}_1 , . . . , \\mathbf{x}_i$, we predict the label $\\mathbf{y}_i$. \n",
    "- Step 1: Embedding, \n",
    "- Step 2: generating $\\alpha$ values using RNN-$\\alpha$, \n",
    "- Step 3: generating $\\mathbf{\\beta}$ values using RNN-$\\beta$, \n",
    "- Step 4: Generating the context vector using attention and representation vectors, \n",
    "- Step 5: Making prediction. \n",
    "\n",
    "Note that in Steps 2 and 3 we use RNN in the reversed time.\n",
    "\n",
    "<img src=./img3/retain-2.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2779a61",
   "metadata": {},
   "source": [
    "<img src=./img3/retain-3.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a624d56d",
   "metadata": {},
   "source": [
    "- **AlphaAttention**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6bef5b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.a_att = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, g):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            g: Tensor of shape (batch_size, seq_length, hidden_dim) \n",
    "               Output of RNN-alpha\n",
    "\n",
    "        Returns:\n",
    "            alpha: Tensor of shape (batch_size, seq_length, 1)\n",
    "                   Normalized attention weights over visits\n",
    "        \"\"\"\n",
    "        a = self.a_att(g)\n",
    "        alpha = torch.softmax(a, dim=1)\n",
    "\n",
    "        return alpha\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb6fd49",
   "metadata": {},
   "source": [
    "- **BetaAttention**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f78468bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BetaAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.b_att = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, h):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            h: Tensor of shape (batch_size, seq_length, hidden_dim)\n",
    "               Output of RNN-beta\n",
    "\n",
    "        Returns:\n",
    "            beta: Tensor of shape (batch_size, seq_length, hidden_dim)\n",
    "                  Feature-wise attention weights\n",
    "        \"\"\"\n",
    "        beta = torch.tanh(self.b_att(h))\n",
    "        return beta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad8d3fb",
   "metadata": {},
   "source": [
    "- **Attention Sum** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8c122e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_sum(alpha, beta, rev_v, rev_masks):\n",
    "    \"\"\"\n",
    "    Compute RETAIN context vector c = sum_t alpha_t * (beta_t ⊙ v_t),\n",
    "    while ignoring padded visits.\n",
    "\n",
    "    Args:\n",
    "        alpha: tensor (B, T, 1)      -- visit-level attention weights\n",
    "        beta:  tensor (B, T, H)      -- feature-level attention vectors\n",
    "        rev_v: tensor (B, T, H)      -- visit embeddings in reversed time\n",
    "        rev_masks: tensor (B, T, C)  -- padded masks (per-code) in reversed time\n",
    "\n",
    "    Returns:\n",
    "        c: tensor (B, H) -- context vector per patient\n",
    "    \"\"\"\n",
    "    v_alpha = beta * alpha     \n",
    "\n",
    "    visit_mask = rev_masks.sum(dim=2) > 0 \n",
    "    \n",
    "    visit_mask_f = visit_mask.unsqueeze(-1).to(dtype=rev_v.dtype)\n",
    "\n",
    "    masked_v = rev_v * visit_mask_f\n",
    "\n",
    "    v_alpha_masked = v_alpha * masked_v\n",
    "\n",
    "    c = v_alpha_masked.sum(dim=1)\n",
    "\n",
    "    return c\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0223c2b",
   "metadata": {},
   "source": [
    "- **Build RETAIN**\n",
    "\n",
    "Now, we can build the RETAIN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e8decae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_embeddings_with_mask(x, masks):\n",
    "    \"\"\"\n",
    "    Mask select the embeddings for true visits (not padding visits) and then sum the embeddings for each visit up.\n",
    "\n",
    "    Arguments:\n",
    "        x: the embeddings of diagnosis sequence of shape (batch_size, # visits, # diagnosis codes, embedding_dim)\n",
    "        masks: the padding masks of shape (batch_size, # visits, # diagnosis codes)\n",
    "\n",
    "    Outputs:\n",
    "        sum_embeddings: the sum of embeddings of shape (batch_size, # visits, embedding_dim)\n",
    "    \"\"\"\n",
    "    \n",
    "    x = x * masks.unsqueeze(-1)\n",
    "    x = torch.sum(x, dim = -2)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52199be7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RETAIN(\n",
       "  (embedding): Embedding(619, 128)\n",
       "  (rnn_a): GRU(128, 128, batch_first=True)\n",
       "  (rnn_b): GRU(128, 128, batch_first=True)\n",
       "  (att_a): AlphaAttention(\n",
       "    (a_att): Linear(in_features=128, out_features=1, bias=True)\n",
       "  )\n",
       "  (att_b): BetaAttention(\n",
       "    (b_att): Linear(in_features=128, out_features=128, bias=True)\n",
       "  )\n",
       "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class RETAIN(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_codes, embedding_dim=128):\n",
    "        super().__init__()\n",
    "        # Define the embedding layer using `nn.Embedding`. Set `embDimSize` to 128.\n",
    "        self.embedding = nn.Embedding(num_codes, embedding_dim)\n",
    "        # Define the RNN-alpha using `nn.GRU()`; Set `hidden_size` to 128. Set `batch_first` to True.\n",
    "        self.rnn_a = nn.GRU(embedding_dim, embedding_dim, batch_first=True)\n",
    "        # Define the RNN-beta using `nn.GRU()`; Set `hidden_size` to 128. Set `batch_first` to True.\n",
    "        self.rnn_b = nn.GRU(embedding_dim, embedding_dim, batch_first=True)\n",
    "        # Define the alpha-attention using `AlphaAttention()`;\n",
    "        self.att_a = AlphaAttention(embedding_dim)\n",
    "        # Define the beta-attention using `BetaAttention()`;\n",
    "        self.att_b = BetaAttention(embedding_dim)\n",
    "        # Define the linear layers using `nn.Linear()`;\n",
    "        self.fc = nn.Linear(embedding_dim, 1)\n",
    "        # Define the final activation layer using `nn.Sigmoid().\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x, masks, rev_x, rev_masks):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            rev_x: the diagnosis sequence in reversed time of shape (# visits, batch_size, # diagnosis codes)\n",
    "            rev_masks: the padding masks in reversed time of shape (# visits, batch_size, # diagnosis codes)\n",
    "\n",
    "        Outputs:\n",
    "            probs: probabilities of shape (batch_size)\n",
    "        \"\"\"\n",
    "        # 1. Pass the reversed sequence through the embedding layer;\n",
    "        rev_x = self.embedding(rev_x)\n",
    "        # 2. Sum the reversed embeddings for each diagnosis code up for a visit of a patient.\n",
    "        rev_x = sum_embeddings_with_mask(rev_x, rev_masks)\n",
    "        # 3. Pass the reversed embegginds through the RNN-alpha and RNN-beta layer separately;\n",
    "        g, _ = self.rnn_a(rev_x)\n",
    "        h, _ = self.rnn_b(rev_x)\n",
    "        # 4. Obtain the alpha and beta attentions using `AlphaAttention()` and `BetaAttention()`;\n",
    "        alpha = self.att_a(g)\n",
    "        beta = self.att_b(h)\n",
    "        # 5. Sum the attention up using `attention_sum()`;\n",
    "        c = attention_sum(alpha, beta, rev_x, rev_masks)\n",
    "        # 6. Pass the context vector through the linear and activation layers.\n",
    "        logits = self.fc(c)\n",
    "        probs = self.sigmoid(logits)\n",
    "        return probs.squeeze()\n",
    "    \n",
    "\n",
    "# load the model here\n",
    "retain = RETAIN(num_codes = len(types))\n",
    "retain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a279a48",
   "metadata": {},
   "source": [
    "## 4. Training and Inferencing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88d1f0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score\n",
    "\n",
    "def eval_model(model, val_loader):\n",
    "    \"\"\"\n",
    "    Evaluate the model.\n",
    "    \n",
    "    Arguments:\n",
    "        model: the RNN model\n",
    "        val_loader: validation dataloader\n",
    "        \n",
    "    Outputs:\n",
    "        precision: overall precision score\n",
    "        recall: overall recall score\n",
    "        f1: overall f1 score\n",
    "        roc_auc: overall roc_auc score\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    y_pred = torch.tensor([], dtype=torch.long)\n",
    "    y_score = torch.tensor([], dtype=torch.float)\n",
    "    y_true = torch.tensor([], dtype=torch.long)\n",
    "\n",
    "    for x, masks, rev_x, rev_masks, y in val_loader:\n",
    "        y_hat = model(x, masks, rev_x, rev_masks)\n",
    "        y_score = torch.cat((y_score, y_hat.detach().cpu()), dim=0)\n",
    "        y_hat_bin = (y_hat > 0.5).int()\n",
    "        y_pred = torch.cat((y_pred, y_hat_bin.detach().cpu()), dim=0)\n",
    "        y_true = torch.cat((y_true, y.detach().cpu()), dim=0)\n",
    "\n",
    "    # Calculate metrics\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')\n",
    "    roc_auc = roc_auc_score(y_true, y_score)\n",
    "\n",
    "    return precision, recall, f1, roc_auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "09354531",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, n_epochs, optimizer, criterion):\n",
    "    \"\"\"\n",
    "    Train the RNN model.\n",
    "\n",
    "    Arguments:\n",
    "        model: the RNN model\n",
    "        train_loader: training dataloader\n",
    "        val_loader: validation dataloader\n",
    "        n_epochs: total number of epochs\n",
    "        optimizer: optimizer\n",
    "        criterion: loss function\n",
    "    \"\"\"\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for x, masks, rev_x, rev_masks, y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            y_hat = model(x, masks, rev_x, rev_masks)\n",
    "            y_hat = y_hat.view(y_hat.shape[0])\n",
    "            \n",
    "            loss = criterion(y_hat, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        print(f'Epoch {epoch+1} \\t Training Loss: {train_loss:.6f}')\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        p, r, f, roc_auc = eval_model(model, val_loader)\n",
    "        print(f'Epoch {epoch+1} \\t Validation Precision: {p:.2f}, Recall: {r:.2f}, F1: {f:.2f}, ROC AUC: {roc_auc:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed46e017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 \t Training Loss: 0.646113\n",
      "Epoch 1 \t Validation Precision: 0.75, Recall: 0.83, F1: 0.78, ROC AUC: 0.83\n",
      "Epoch 2 \t Training Loss: 0.469833\n",
      "Epoch 2 \t Validation Precision: 0.77, Recall: 0.74, F1: 0.75, ROC AUC: 0.84\n",
      "Epoch 3 \t Training Loss: 0.293150\n",
      "Epoch 3 \t Validation Precision: 0.78, Recall: 0.81, F1: 0.79, ROC AUC: 0.83\n",
      "Epoch 4 \t Training Loss: 0.148171\n",
      "Epoch 4 \t Validation Precision: 0.77, Recall: 0.82, F1: 0.79, ROC AUC: 0.84\n",
      "Epoch 5 \t Training Loss: 0.068694\n",
      "Epoch 5 \t Validation Precision: 0.82, Recall: 0.79, F1: 0.80, ROC AUC: 0.85\n"
     ]
    }
   ],
   "source": [
    "# load the model\n",
    "retain = RETAIN(num_codes = len(types))\n",
    "\n",
    "# load the loss function\n",
    "criterion = nn.BCELoss()\n",
    "# load the optimizer\n",
    "optimizer = torch.optim.Adam(retain.parameters(), lr=1e-3)\n",
    "# number of epochs to train the model\n",
    "n_epochs = 5\n",
    "train(retain, train_loader, val_loader, n_epochs=5, optimizer=optimizer, criterion=criterion)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb3f069",
   "metadata": {},
   "source": [
    "## 5. Sensitivity analysis\n",
    "\n",
    "I will train the same model but with different hyperparameters. I will be using 0.1 and 0.001 for learning rate, and 16, 128 for embedding dimensions. It shows how model performance varies with different values of learning rate and embedding dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48e93c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "{'learning rate': 0.1, 'embedding_dim': 8}\n",
      "--------------------------------------------------\n",
      "Epoch 1 \t Training Loss: 0.671263\n",
      "Epoch 1 \t Validation Precision: 0.72, Recall: 0.66, F1: 0.69, ROC AUC: 0.78\n",
      "Epoch 2 \t Training Loss: 0.562615\n",
      "Epoch 2 \t Validation Precision: 0.69, Recall: 0.84, F1: 0.76, ROC AUC: 0.81\n",
      "Epoch 3 \t Training Loss: 0.531844\n",
      "Epoch 3 \t Validation Precision: 0.66, Recall: 0.82, F1: 0.73, ROC AUC: 0.78\n",
      "Epoch 4 \t Training Loss: 0.491145\n",
      "Epoch 4 \t Validation Precision: 0.81, Recall: 0.56, F1: 0.66, ROC AUC: 0.78\n",
      "Epoch 5 \t Training Loss: 0.464876\n",
      "Epoch 5 \t Validation Precision: 0.68, Recall: 0.78, F1: 0.72, ROC AUC: 0.77\n",
      "Epoch 1 \t Training Loss: 0.503799\n",
      "Epoch 1 \t Validation Precision: 0.69, Recall: 0.81, F1: 0.74, ROC AUC: 0.76\n",
      "Epoch 2 \t Training Loss: 0.456363\n",
      "Epoch 2 \t Validation Precision: 0.69, Recall: 0.85, F1: 0.76, ROC AUC: 0.79\n",
      "Epoch 3 \t Training Loss: 0.390751\n",
      "Epoch 3 \t Validation Precision: 0.70, Recall: 0.65, F1: 0.67, ROC AUC: 0.76\n",
      "Epoch 4 \t Training Loss: 0.378248\n",
      "Epoch 4 \t Validation Precision: 0.69, Recall: 0.75, F1: 0.72, ROC AUC: 0.77\n",
      "Epoch 5 \t Training Loss: 0.397652\n",
      "Epoch 5 \t Validation Precision: 0.73, Recall: 0.71, F1: 0.72, ROC AUC: 0.78\n",
      "==================================================\n",
      "{'learning rate': 0.1, 'embedding_dim': 128}\n",
      "--------------------------------------------------\n",
      "Epoch 1 \t Training Loss: 3.009634\n",
      "Epoch 1 \t Validation Precision: 0.53, Recall: 0.99, F1: 0.69, ROC AUC: 0.58\n",
      "Epoch 2 \t Training Loss: 1.670546\n",
      "Epoch 2 \t Validation Precision: 0.52, Recall: 0.99, F1: 0.68, ROC AUC: 0.58\n",
      "Epoch 3 \t Training Loss: 1.901983\n",
      "Epoch 3 \t Validation Precision: 0.52, Recall: 0.98, F1: 0.68, ROC AUC: 0.51\n",
      "Epoch 4 \t Training Loss: 3.201386\n",
      "Epoch 4 \t Validation Precision: 0.51, Recall: 0.99, F1: 0.68, ROC AUC: 0.50\n",
      "Epoch 5 \t Training Loss: 2.130802\n",
      "Epoch 5 \t Validation Precision: 0.51, Recall: 0.95, F1: 0.66, ROC AUC: 0.49\n",
      "Epoch 1 \t Training Loss: 1.881748\n",
      "Epoch 1 \t Validation Precision: 0.51, Recall: 0.91, F1: 0.66, ROC AUC: 0.50\n",
      "Epoch 2 \t Training Loss: 2.010188\n",
      "Epoch 2 \t Validation Precision: 0.51, Recall: 0.91, F1: 0.66, ROC AUC: 0.50\n",
      "Epoch 3 \t Training Loss: 2.168853\n",
      "Epoch 3 \t Validation Precision: 0.51, Recall: 0.91, F1: 0.66, ROC AUC: 0.50\n",
      "Epoch 4 \t Training Loss: 2.288784\n",
      "Epoch 4 \t Validation Precision: 0.51, Recall: 0.91, F1: 0.66, ROC AUC: 0.49\n",
      "Epoch 5 \t Training Loss: 1.900206\n",
      "Epoch 5 \t Validation Precision: 0.51, Recall: 0.90, F1: 0.65, ROC AUC: 0.58\n",
      "==================================================\n",
      "{'learning rate': 0.001, 'embedding_dim': 8}\n",
      "--------------------------------------------------\n",
      "Epoch 1 \t Training Loss: 0.735420\n",
      "Epoch 1 \t Validation Precision: 0.47, Recall: 0.20, F1: 0.28, ROC AUC: 0.40\n",
      "Epoch 2 \t Training Loss: 0.717718\n",
      "Epoch 2 \t Validation Precision: 0.45, Recall: 0.24, F1: 0.31, ROC AUC: 0.43\n",
      "Epoch 3 \t Training Loss: 0.706089\n",
      "Epoch 3 \t Validation Precision: 0.50, Recall: 0.30, F1: 0.38, ROC AUC: 0.46\n",
      "Epoch 4 \t Training Loss: 0.697697\n",
      "Epoch 4 \t Validation Precision: 0.50, Recall: 0.39, F1: 0.44, ROC AUC: 0.51\n",
      "Epoch 5 \t Training Loss: 0.689633\n",
      "Epoch 5 \t Validation Precision: 0.55, Recall: 0.56, F1: 0.56, ROC AUC: 0.55\n",
      "Epoch 1 \t Training Loss: 0.682684\n",
      "Epoch 1 \t Validation Precision: 0.57, Recall: 0.64, F1: 0.60, ROC AUC: 0.59\n",
      "Epoch 2 \t Training Loss: 0.674185\n",
      "Epoch 2 \t Validation Precision: 0.58, Recall: 0.71, F1: 0.64, ROC AUC: 0.62\n",
      "Epoch 3 \t Training Loss: 0.665799\n",
      "Epoch 3 \t Validation Precision: 0.59, Recall: 0.81, F1: 0.68, ROC AUC: 0.66\n",
      "Epoch 4 \t Training Loss: 0.654937\n",
      "Epoch 4 \t Validation Precision: 0.60, Recall: 0.83, F1: 0.70, ROC AUC: 0.69\n",
      "Epoch 5 \t Training Loss: 0.643375\n",
      "Epoch 5 \t Validation Precision: 0.61, Recall: 0.85, F1: 0.71, ROC AUC: 0.71\n",
      "==================================================\n",
      "{'learning rate': 0.001, 'embedding_dim': 128}\n",
      "--------------------------------------------------\n",
      "Epoch 1 \t Training Loss: 0.646922\n",
      "Epoch 1 \t Validation Precision: 0.69, Recall: 0.79, F1: 0.73, ROC AUC: 0.79\n",
      "Epoch 2 \t Training Loss: 0.439476\n",
      "Epoch 2 \t Validation Precision: 0.71, Recall: 0.80, F1: 0.75, ROC AUC: 0.80\n",
      "Epoch 3 \t Training Loss: 0.265159\n",
      "Epoch 3 \t Validation Precision: 0.72, Recall: 0.84, F1: 0.78, ROC AUC: 0.80\n",
      "Epoch 4 \t Training Loss: 0.128337\n",
      "Epoch 4 \t Validation Precision: 0.71, Recall: 0.75, F1: 0.73, ROC AUC: 0.78\n",
      "Epoch 5 \t Training Loss: 0.062657\n",
      "Epoch 5 \t Validation Precision: 0.73, Recall: 0.80, F1: 0.76, ROC AUC: 0.79\n",
      "Epoch 1 \t Training Loss: 0.034961\n",
      "Epoch 1 \t Validation Precision: 0.72, Recall: 0.82, F1: 0.76, ROC AUC: 0.79\n",
      "Epoch 2 \t Training Loss: 0.018966\n",
      "Epoch 2 \t Validation Precision: 0.71, Recall: 0.84, F1: 0.77, ROC AUC: 0.79\n",
      "Epoch 3 \t Training Loss: 0.012166\n",
      "Epoch 3 \t Validation Precision: 0.72, Recall: 0.81, F1: 0.76, ROC AUC: 0.79\n",
      "Epoch 4 \t Training Loss: 0.008827\n",
      "Epoch 4 \t Validation Precision: 0.72, Recall: 0.81, F1: 0.76, ROC AUC: 0.79\n",
      "Epoch 5 \t Training Loss: 0.007073\n",
      "Epoch 5 \t Validation Precision: 0.72, Recall: 0.80, F1: 0.76, ROC AUC: 0.79\n"
     ]
    }
   ],
   "source": [
    "lr_hyperparameter = [1e-1, 1e-3]\n",
    "embedding_dim_hyperparameter = [8, 128]\n",
    "n_epochs = 5\n",
    "results = {}\n",
    "\n",
    "for lr in lr_hyperparameter:\n",
    "    for embedding_dim in embedding_dim_hyperparameter:\n",
    "        print ('='*50)\n",
    "        print ({'learning rate': lr, \"embedding_dim\": embedding_dim})\n",
    "        print ('-'*50)\n",
    "        \"\"\" \n",
    "        TODO: \n",
    "            1. Load the model by specifying `embedding_dim` as input to RETAIN. It will create different model with different embedding dimension.\n",
    "            2. Load the loss function `nn.BCELoss`\n",
    "            3. Load the optimizer `torch.optim.Adam` with learning rate using `lr` variable\n",
    "        \"\"\"\n",
    "        # load the model\n",
    "        retain = RETAIN(len(types), embedding_dim)\n",
    "\n",
    "        # load the loss function\n",
    "        criterion = nn.BCELoss()\n",
    "        # load the optimizer\n",
    "        optimizer = torch.optim.Adam(retain.parameters(), lr)\n",
    "        n_epochs = 5\n",
    "        train(retain, train_loader, val_loader, n_epochs=5, optimizer=optimizer, criterion=criterion)\n",
    "        roc_auc = train(retain, train_loader, val_loader, n_epochs=5, optimizer=optimizer, criterion=criterion)\n",
    "        results['lr:{},emb:{}'.format(str(lr), str(embedding_dim))] =  roc_auc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
