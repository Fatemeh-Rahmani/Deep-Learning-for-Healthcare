{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90f211e5",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network (RNN)\n",
    "\n",
    "## 1. Overview\n",
    "\n",
    "In this homework, we will build a **bi-directional RNN** to predict **Heart Failure** from patients' diagnosis codes.  \n",
    "The recurrent nature of RNNs allows modeling **temporal relationships** between multiple visits of a patient.\n",
    "\n",
    "### About Raw Data\n",
    "\n",
    "- Dataset: Synthetic data based on [MIMIC-III](https://mimic.physionet.org/gettingstarted/access/)  \n",
    "- Input: Sequences of diagnosis codes for each patient  \n",
    "- Task: Predict Heart Failure occurrence  \n",
    "- Data is already preprocessed and ready to be loaded into the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad6b3318",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# Set seed\n",
    "seed = 24\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "# Define data path\n",
    "DATA_PATH = \"data2\"\n",
    "\n",
    "# Suppress warnings about ragged sequences\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', message='.*creating an ndarray from ragged nested sequences.*')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e26a416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of patients (pids): 1000\n",
      "Number of visits (vids): 1000\n",
      "Number of heart failure labels (hfs): 1000\n",
      "Number of sequences (seqs): 1000\n",
      "Number of diagnosis types (types): 619\n",
      "Number of related types (rtypes): 619\n"
     ]
    }
   ],
   "source": [
    "pids = pickle.load(open(os.path.join(DATA_PATH,'train/pids.pkl'), 'rb')) \n",
    "vids = pickle.load(open(os.path.join(DATA_PATH,'train/vids.pkl'), 'rb'))\n",
    "hfs = pickle.load(open(os.path.join(DATA_PATH,'train/hfs.pkl'), 'rb'))\n",
    "seqs = pickle.load(open(os.path.join(DATA_PATH,'train/seqs.pkl'), 'rb'))\n",
    "types = pickle.load(open(os.path.join(DATA_PATH,'train/types.pkl'), 'rb'))\n",
    "rtypes = pickle.load(open(os.path.join(DATA_PATH,'train/rtypes.pkl'), 'rb'))\n",
    "\n",
    "# Print lengths to verify\n",
    "print(\"Number of patients (pids):\", len(pids))\n",
    "print(\"Number of visits (vids):\", len(vids))\n",
    "print(\"Number of heart failure labels (hfs):\", len(hfs))\n",
    "print(\"Number of sequences (seqs):\", len(seqs))\n",
    "print(\"Number of diagnosis types (types):\", len(types))\n",
    "print(\"Number of related types (rtypes):\", len(rtypes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f40eaef",
   "metadata": {},
   "source": [
    "where\n",
    "\n",
    "- `pids`: contains the patient ids\n",
    "- `vids`: contains a list of visit ids for each patient\n",
    "- `hfs`: contains the heart failure label (0: normal, 1: heart failure) for each patient\n",
    "- `seqs`: contains a list of visit (in ICD9 codes) for each patient\n",
    "- `types`: contains the map from ICD9 codes to ICD-9 labels\n",
    "- `rtypes`: contains the map from ICD9 labels to ICD9 codes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69311177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patient ID: 47537\n",
      "Heart Failure: 0\n",
      "# of visits: 2\n",
      "\t0-th visit id: 0\n",
      "\t0-th visit diagnosis labels: [12, 103, 262, 285, 290, 292, 359, 416, 39, 225, 275, 294, 326, 267, 93]\n",
      "\t0-th visit diagnosis codes: ['DIAG_041', 'DIAG_276', 'DIAG_518', 'DIAG_560', 'DIAG_567', 'DIAG_569', 'DIAG_707', 'DIAG_785', 'DIAG_155', 'DIAG_456', 'DIAG_537', 'DIAG_571', 'DIAG_608', 'DIAG_529', 'DIAG_263']\n",
      "\t1-th visit id: 1\n",
      "\t1-th visit diagnosis labels: [12, 103, 240, 262, 290, 292, 319, 359, 510, 513, 577, 307, 8, 280, 18, 131]\n",
      "\t1-th visit diagnosis codes: ['DIAG_041', 'DIAG_276', 'DIAG_482', 'DIAG_518', 'DIAG_567', 'DIAG_569', 'DIAG_599', 'DIAG_707', 'DIAG_995', 'DIAG_998', 'DIAG_V09', 'DIAG_584', 'DIAG_031', 'DIAG_553', 'DIAG_070', 'DIAG_305']\n"
     ]
    }
   ],
   "source": [
    "# take the 3rd patient as an example\n",
    "\n",
    "print(\"Patient ID:\", pids[3])\n",
    "print(\"Heart Failure:\", hfs[3])\n",
    "print(\"# of visits:\", len(vids[3]))\n",
    "for visit in range(len(vids[3])):\n",
    "    print(f\"\\t{visit}-th visit id:\", vids[3][visit])\n",
    "    print(f\"\\t{visit}-th visit diagnosis labels:\", seqs[3][visit])\n",
    "    print(f\"\\t{visit}-th visit diagnosis codes:\", [rtypes[label] for label in seqs[3][visit]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc52cd25",
   "metadata": {},
   "source": [
    "### Data Overview\n",
    "\n",
    "- `seqs` is a **3-level nested list**:  \n",
    "  - `seqs[i][j][k]` gives the **k-th diagnosis code** for the **j-th visit** of the **i-th patient**.  \n",
    "  - Example: `seqs[0][0]` → diagnosis codes of the **first visit of the first patient**.  \n",
    "  - ICD9 codes like `DIAG_276` can be looked up online (e.g., *disorders of fluid electrolyte and acid-base balance*).\n",
    "\n",
    "- `hfs` is a **list of heart failure labels**:  \n",
    "  - `1` → patient has heart failure  \n",
    "  - `0` → patient does not have heart failure  \n",
    "\n",
    "- **Number of heart failure patients in the training set:**  \n",
    "  - `sum(hfs)` gives the total number of HF patients  \n",
    "  - Fraction of HF patients: `sum(hfs) / len(hfs)`  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93a6d10b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of heart failure patients: 548\n",
      "ratio of heart failure patients: 0.55\n"
     ]
    }
   ],
   "source": [
    "print(\"number of heart failure patients:\", sum(hfs))\n",
    "print(\"ratio of heart failure patients: %.2f\" % (sum(hfs) / len(hfs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3a4554",
   "metadata": {},
   "source": [
    "## 2. Build the dataset\n",
    "\n",
    "### - CustomDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddadbcfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of patients in dataset: 1000\n",
      "Example patient first visit: [[85, 112, 346, 380, 269, 511, 114, 103, 530, 597, 511], [85, 103, 112, 513, 511, 19, 149, 530, 186, 66]]\n",
      "Example patient label: 1\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, seqs, hfs):\n",
    "        \"\"\"\n",
    "        Store seqs and hfs as lists. Do NOT convert to np.array since sequences are ragged.\n",
    "        \"\"\"\n",
    "        self.x = seqs  # keep as list\n",
    "        self.y = hfs\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        x = self.x[index]\n",
    "        y = self.y[index]\n",
    "        return x, y\n",
    "\n",
    "# Create dataset instance\n",
    "dataset = CustomDataset(seqs, hfs)\n",
    "\n",
    "# Quick check\n",
    "print(\"Number of patients in dataset:\", len(dataset))\n",
    "print(\"Example patient first visit:\", dataset[0][0])\n",
    "print(\"Example patient label:\", dataset[0][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb33bb83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "dataset = CustomDataset(seqs, hfs)\n",
    "\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1063ad18",
   "metadata": {},
   "source": [
    "### Collate Function\n",
    "\n",
    "In this section, we define a **collate function (`collate_fn`)** for batch training RNNs on patient diagnosis sequences.\n",
    "\n",
    "**Key points:**\n",
    "\n",
    "1. **Variable-length sequences:**  \n",
    "   - Each patient has multiple visits, each with a varying number of diagnosis codes.  \n",
    "   - Example: `seqs[i][j][k]` → k-th code of j-th visit of i-th patient.\n",
    "\n",
    "2. **Padding:**  \n",
    "   - Pad visits and diagnosis codes with `0` so all patients in a batch have the same shape.  \n",
    "   - Ensures tensor compatibility for batch processing.\n",
    "\n",
    "3. **Masking:**  \n",
    "   - Create a mask with `1` for original codes and `0` for padded values.  \n",
    "   - Allows the model to **ignore padded values** during training.\n",
    "\n",
    "4. **Reversed sequences:**  \n",
    "   - Flip visits in time (only true visits) for bi-directional RNNs.  \n",
    "   - Create a reversed mask correspondingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6ee6eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    \"\"\"\n",
    "    Collate a list of samples into a batch for RNN training.\n",
    "\n",
    "    Args:\n",
    "        data: list of samples from CustomDataset, each (sequence, label)\n",
    "    \n",
    "    Returns:\n",
    "        x: tensor (#patients, max_visits, max_codes), type=torch.long\n",
    "        masks: tensor (#patients, max_visits, max_codes), type=torch.bool\n",
    "        rev_x: same as x but reversed in time\n",
    "        rev_masks: same as masks but reversed in time\n",
    "        y: tensor (#patients), type=torch.float\n",
    "    \"\"\"\n",
    "    sequences, labels = zip(*data)\n",
    "    y = torch.tensor(labels, dtype=torch.float)\n",
    "    \n",
    "    num_patients = len(sequences)\n",
    "    max_num_visits = max(len(patient) for patient in sequences)\n",
    "    max_num_codes = max(len(visit) for patient in sequences for visit in patient)\n",
    "    \n",
    "    x = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.long)\n",
    "    rev_x = torch.zeros_like(x)\n",
    "    masks = torch.zeros_like(x, dtype=torch.bool)\n",
    "    rev_masks = torch.zeros_like(x, dtype=torch.bool)\n",
    "    \n",
    "    for i_patient, patient in enumerate(sequences):\n",
    "        for j_visit, visit in enumerate(patient):\n",
    "            visit_tensor = torch.tensor(visit, dtype=torch.long)\n",
    "            x[i_patient, j_visit, :len(visit)] = visit_tensor\n",
    "            rev_x[i_patient, len(patient)-j_visit-1, :len(visit)] = visit_tensor\n",
    "            masks[i_patient, j_visit, :len(visit)] = True\n",
    "            rev_masks[i_patient, len(patient)-j_visit-1, :len(visit)] = True\n",
    "    \n",
    "    return x, masks, rev_x, rev_masks, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea67f556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n",
      "torch.float32\n",
      "torch.bool\n",
      "torch.Size([10, 3, 24])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=10, collate_fn=collate_fn)\n",
    "loader_iter = iter(loader)\n",
    "x, masks, rev_x, rev_masks, y = next(loader_iter)\n",
    "\n",
    "print(x.dtype)\n",
    "print(y.dtype)\n",
    "print(masks.dtype)\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91f5c97",
   "metadata": {},
   "source": [
    "Now we have `CustomDataset` and `collate_fn()`. I split the dataset into training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd0b2823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train dataset: 800\n",
      "Length of val dataset: 200\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "split = int(len(dataset)*0.8)\n",
    "\n",
    "lengths = [split, len(dataset) - split]\n",
    "train_dataset, val_dataset = random_split(dataset, lengths)\n",
    "\n",
    "print(\"Length of train dataset:\", len(train_dataset))\n",
    "print(\"Length of val dataset:\", len(val_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cae10d",
   "metadata": {},
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6000f791",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def load_data(train_dataset, val_dataset, collate_fn):\n",
    "    \n",
    "    batch_size = 32\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=collate_fn)\n",
    "    \n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "train_loader, val_loader = load_data(train_dataset, val_dataset, collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c13a8e",
   "metadata": {},
   "source": [
    "## 3. Naive Bi-directional RNN\n",
    "\n",
    "In this section, I implement a **naive bi-directional RNN** for heart failure prediction.\n",
    "\n",
    "**Key steps:**\n",
    "\n",
    "1. **Embedding layer:**  \n",
    "   - Transform diagnosis codes of each visit into dense vectors.  \n",
    "   - Use `nn.Embedding(num_embeddings, embedding_dim)`.  \n",
    "   - `num_embeddings` = total number of unique diagnosis codes.  \n",
    "   - `embedding_dim` = size of the embedding vector for each code.\n",
    "\n",
    "2. **Bi-directional RNN:**  \n",
    "   - Processes the embedded sequences in both forward and backward directions.  \n",
    "   - Captures temporal dependencies across visits in both directions.\n",
    "\n",
    "3. **Output:**  \n",
    "   - Aggregated hidden states → Linear layer → Binary classification (heart failure: yes/no).\n",
    "\n",
    "\n",
    "**Summary:**  \n",
    "- Each patient sequence → Embedding → Bi-RNN → Linear → Prediction.  \n",
    "- Using masks ensures that padded visits/codes do not affect the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6955db",
   "metadata": {},
   "source": [
    "- **3.1 Mask Selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c319104",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_embeddings_with_mask(x, masks):\n",
    " \n",
    "    masks = masks.unsqueeze(-1)\n",
    "    mask_embeddings = x * masks\n",
    "    result = mask_embeddings.sum(dim=2)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92ba73a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import ast\n",
    "import inspect\n",
    "\n",
    "\n",
    "def uses_loop(function):\n",
    "    loop_statements = ast.For, ast.While, ast.AsyncFor\n",
    "\n",
    "    nodes = ast.walk(ast.parse(inspect.getsource(function)))\n",
    "    return any(isinstance(node, loop_statements) for node in nodes)\n",
    "\n",
    "def generate_random_mask(batch_size, max_num_visits , max_num_codes):\n",
    "    num_visits = [random.randint(1, max_num_visits) for _ in range(batch_size)]\n",
    "    num_codes = []\n",
    "    for n in num_visits:\n",
    "        num_codes_visit = [0] * max_num_visits\n",
    "        for i in range(n):\n",
    "            num_codes_visit[i] = (random.randint(1, max_num_codes))\n",
    "        num_codes.append(num_codes_visit)\n",
    "    masks = [torch.ones((l,), dtype=torch.bool) for num_codes_visit in num_codes for l in num_codes_visit]\n",
    "    masks = torch.stack([torch.cat([i, i.new_zeros(max_num_codes - i.size(0))], 0) for i in masks], 0)\n",
    "    masks = masks.view((batch_size, max_num_visits, max_num_codes)).bool()\n",
    "    return masks\n",
    "\n",
    "\n",
    "batch_size = 16\n",
    "max_num_visits = 10\n",
    "max_num_codes = 20\n",
    "embedding_dim = 100\n",
    "\n",
    "torch.random.manual_seed(7)\n",
    "x = torch.randn((batch_size, max_num_visits , max_num_codes, embedding_dim))\n",
    "masks = generate_random_mask(batch_size, max_num_visits , max_num_codes)\n",
    "out = sum_embeddings_with_mask(x, masks)\n",
    "\n",
    "assert uses_loop(sum_embeddings_with_mask) is False\n",
    "assert out.shape == (batch_size, max_num_visits, embedding_dim)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88a250e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_visit(hidden_states, masks):\n",
    "    \"\"\"\n",
    "    Obtain the hidden state for the last true visit (not padding visits).\n",
    "\n",
    "    Arguments:\n",
    "        hidden_states: tensor of shape (batch_size, # visits, embedding_dim)\n",
    "        masks: tensor of shape (batch_size, # visits, # diagnosis codes)\n",
    "\n",
    "    Outputs:\n",
    "        last_hidden_state: tensor of shape (batch_size, embedding_dim)\n",
    "    \"\"\" \n",
    "    visit_mask = masks.any(dim=2)  \n",
    "    lengths = visit_mask.sum(dim=1) \n",
    "    batch_indices = torch.arange(hidden_states.size(0))\n",
    "    last_hidden_state = hidden_states[batch_indices, lengths-1, :]   \n",
    "    \n",
    "    return last_hidden_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6761119c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "torch.Size([16, 100])\n"
     ]
    }
   ],
   "source": [
    "print(uses_loop(get_last_visit))\n",
    "\n",
    "max_num_visits = 10\n",
    "batch_size = 16\n",
    "max_num_codes = 20\n",
    "embedding_dim = 100\n",
    "\n",
    "torch.random.manual_seed(7)\n",
    "hidden_states = torch.randn((batch_size, max_num_visits, embedding_dim))\n",
    "masks = generate_random_mask(batch_size, max_num_visits , max_num_codes)\n",
    "out = get_last_visit(hidden_states, masks)\n",
    "\n",
    "print(out.shape) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ca9119",
   "metadata": {},
   "source": [
    "- **3.2 Build NaiveRNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d4f71204",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NaiveRNN(\n",
       "  (embedding): Embedding(619, 128)\n",
       "  (rnn): GRU(128, 128, batch_first=True)\n",
       "  (rev_rnn): GRU(128, 128, batch_first=True)\n",
       "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NaiveRNN(nn.Module):\n",
    "    def __init__(self, num_codes):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=num_codes, embedding_dim=128)\n",
    "        self.rnn = nn.GRU(input_size=128, hidden_size=128, batch_first=True)\n",
    "        self.rev_rnn = nn.GRU(input_size=128, hidden_size=128, batch_first=True)\n",
    "        self.fc = nn.Linear(256, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x, masks, rev_x, rev_masks):\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        # Forward direction\n",
    "        x = self.embedding(x)\n",
    "        x = sum_embeddings_with_mask(x, masks)\n",
    "        output, _ = self.rnn(x)\n",
    "        true_h_n = get_last_visit(output, masks)\n",
    "        \n",
    "        # Reverse direction\n",
    "        rev_x = self.embedding(rev_x)\n",
    "        rev_x = sum_embeddings_with_mask(rev_x, rev_masks)\n",
    "        output_rev, _ = self.rev_rnn(rev_x)\n",
    "        true_h_n_rev = get_last_visit(output_rev, rev_masks)\n",
    "        \n",
    "        # Concatenate hidden states and pass through linear + sigmoid\n",
    "        logits = self.fc(torch.cat([true_h_n, true_h_n_rev], dim=1))\n",
    "        probs = self.sigmoid(logits)\n",
    "        return probs.view(batch_size)\n",
    "    \n",
    "\n",
    "naive_rnn = NaiveRNN(num_codes=len(types))\n",
    "naive_rnn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2095107d",
   "metadata": {},
   "source": [
    "## 4.  Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a436f004",
   "metadata": {},
   "source": [
    "**Loss and Optimizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf382356",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(naive_rnn.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d74a30f",
   "metadata": {},
   "source": [
    "**Evaluate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73cb995b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score\n",
    "import torch\n",
    "\n",
    "def eval_model(model, val_loader):\n",
    "    \"\"\"\n",
    "    Evaluate the model.\n",
    "    \n",
    "    Arguments:\n",
    "        model: the RNN model\n",
    "        val_loader: validation dataloader\n",
    "        \n",
    "    Outputs:\n",
    "        precision: overall precision score\n",
    "        recall: overall recall score\n",
    "        f1: overall f1 score\n",
    "        roc_auc: overall roc_auc score\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    y_pred = torch.tensor([], dtype=torch.long)\n",
    "    y_score = torch.tensor([], dtype=torch.float)\n",
    "    y_true = torch.tensor([], dtype=torch.long)\n",
    "\n",
    "    for x, masks, rev_x, rev_masks, y in val_loader:\n",
    "        y_hat = model(x, masks, rev_x, rev_masks)\n",
    "        y_score = torch.cat((y_score, y_hat.detach().cpu()), dim=0)\n",
    "        y_hat_bin = (y_hat > 0.5).int()\n",
    "        y_pred = torch.cat((y_pred, y_hat_bin.detach().cpu()), dim=0)\n",
    "        y_true = torch.cat((y_true, y.detach().cpu()), dim=0)\n",
    "\n",
    "    # Calculate metrics\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')\n",
    "    roc_auc = roc_auc_score(y_true, y_score)\n",
    "\n",
    "    return precision, recall, f1, roc_auc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fc94d8",
   "metadata": {},
   "source": [
    "**Training and evlauation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c9a3b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, n_epochs, optimizer, criterion):\n",
    "    \"\"\"\n",
    "    Train the RNN model.\n",
    "\n",
    "    Arguments:\n",
    "        model: the RNN model\n",
    "        train_loader: training dataloader\n",
    "        val_loader: validation dataloader\n",
    "        n_epochs: total number of epochs\n",
    "        optimizer: optimizer\n",
    "        criterion: loss function\n",
    "    \"\"\"\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for x, masks, rev_x, rev_masks, y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            y_hat = model(x, masks, rev_x, rev_masks)\n",
    "            y_hat = y_hat.view(y_hat.shape[0])\n",
    "            \n",
    "            loss = criterion(y_hat, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        print(f'Epoch {epoch+1} \\t Training Loss: {train_loss:.6f}')\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        p, r, f, roc_auc = eval_model(model, val_loader)\n",
    "        print(f'Epoch {epoch+1} \\t Validation Precision: {p:.2f}, Recall: {r:.2f}, F1: {f:.2f}, ROC AUC: {roc_auc:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "545b8820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 \t Training Loss: 0.616105\n",
      "Epoch 1 \t Validation Precision: 0.71, Recall: 0.89, F1: 0.79, ROC AUC: 0.84\n",
      "Epoch 2 \t Training Loss: 0.436958\n",
      "Epoch 2 \t Validation Precision: 0.71, Recall: 0.84, F1: 0.77, ROC AUC: 0.84\n",
      "Epoch 3 \t Training Loss: 0.331513\n",
      "Epoch 3 \t Validation Precision: 0.73, Recall: 0.80, F1: 0.76, ROC AUC: 0.85\n",
      "Epoch 4 \t Training Loss: 0.227049\n",
      "Epoch 4 \t Validation Precision: 0.73, Recall: 0.85, F1: 0.79, ROC AUC: 0.85\n",
      "Epoch 5 \t Training Loss: 0.141484\n",
      "Epoch 5 \t Validation Precision: 0.74, Recall: 0.86, F1: 0.79, ROC AUC: 0.85\n"
     ]
    }
   ],
   "source": [
    "# number of epochs to train the model\n",
    "n_epochs = 5\n",
    "train(naive_rnn, train_loader, val_loader, n_epochs=5, optimizer=optimizer, criterion=criterion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "159c8769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.7355371900826446\n",
      "Recall: 0.8640776699029126\n",
      "F1 Score: 0.7946428571428571\n",
      "ROC AUC: 0.8537683915523971\n"
     ]
    }
   ],
   "source": [
    "p, r, f, roc_auc = eval_model(naive_rnn, val_loader)\n",
    "\n",
    "print(\"Precision:\", p)\n",
    "print(\"Recall:\", r)\n",
    "print(\"F1 Score:\", f)\n",
    "print(\"ROC AUC:\", roc_auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70fcdc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
